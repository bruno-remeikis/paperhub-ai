# """
# Servi√ßo de Revis√£o de Documentos Acad√™micos usando Agno
# Resolve problemas de: documentos longos, perda de contexto e a√ß√µes globais
# """

# import json
# import re
# from typing import List, Dict, Any, Optional, Tuple
# from dataclasses import dataclass, asdict
# from enum import Enum
# import hashlib
# from bs4 import BeautifulSoup
# import math

# # Assumindo que o Agno est√° instalado - estrutura baseada na documenta√ß√£o
# from agno.agent import Agent
# from agno.models.openai import OpenAIChat
# from agno.storage.postgres import PostgresKnowledgeBase
# from agno.tools import Tool


# class ProcessingStrategy(Enum):
#     SINGLE_PASS = "single_pass"
#     CHUNKED = "chunked" 
#     HIERARCHICAL = "hierarchical"


# @dataclass
# class DocumentChunk:
#     """Representa um chunk do documento com metadados"""
#     id: str
#     content: str
#     position: int
#     section_title: str
#     context_summary: str
#     word_count: int
    
    
# @dataclass
# class DocumentContext:
#     """Contexto global do documento"""
#     title: str
#     abstract: str
#     sections: List[str]
#     main_topics: List[str]
#     writing_style: str
#     academic_level: str
#     total_words: int
    

# @dataclass
# class Suggestion:
#     """Sugest√£o de altera√ß√£o no documento"""
#     change: str
#     explanation: str
#     chunk_id: Optional[str] = None
#     global_action: bool = False
    

# @dataclass
# class ProcessingResult:
#     """Resultado final do processamento"""
#     suggestions: List[Suggestion]
#     answer: str
#     modified_document: str
#     strategy_used: ProcessingStrategy
#     cost_estimate: float


# class DocumentAnalyzer:
#     """Analisa documentos e determina estrat√©gia de processamento"""
    
#     # Limites para estrat√©gias (em caracteres)
#     SINGLE_PASS_LIMIT = 15000  # ~4000 tokens
#     CHUNK_SIZE = 8000  # ~2000 tokens por chunk
#     OVERLAP_SIZE = 1000  # Overlap entre chunks
    
#     def __init__(self):
#         self.model = OpenAIChat(id="gpt-4o-mini")  # Modelo barato para an√°lise
        
#     def analyze_document(self, html_content: str) -> Tuple[ProcessingStrategy, DocumentContext]:
#         """Analisa documento e determina estrat√©gia de processamento"""
        
#         # Parse HTML para an√°lise
#         soup = BeautifulSoup(html_content, 'html.parser')
#         text_content = soup.get_text()
        
#         char_count = len(html_content)
#         word_count = len(text_content.split())
        
#         # Extrai informa√ß√µes estruturais
#         sections = self._extract_sections(soup)
#         context = self._extract_context(soup, text_content)
        
#         # Determina estrat√©gia baseada no tamanho
#         if char_count <= self.SINGLE_PASS_LIMIT:
#             strategy = ProcessingStrategy.SINGLE_PASS
#         elif char_count <= 100000:  # ~25k tokens
#             strategy = ProcessingStrategy.CHUNKED
#         else:
#             strategy = ProcessingStrategy.HIERARCHICAL
            
#         return strategy, DocumentContext(
#             title=self._extract_title(soup),
#             abstract=self._extract_abstract(soup),
#             sections=sections,
#             main_topics=context['topics'],
#             writing_style=context['style'],
#             academic_level=context['level'],
#             total_words=word_count
#         )
    
#     def create_chunks(self, html_content: str, context: DocumentContext) -> List[DocumentChunk]:
#         """Cria chunks inteligentes do documento"""
        
#         soup = BeautifulSoup(html_content, 'html.parser')
#         chunks = []
        
#         # Primeira tentativa: dividir por se√ß√µes
#         sections = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
#         if len(sections) > 1:
#             chunks = self._chunk_by_sections(soup, sections, context)
#         else:
#             chunks = self._chunk_by_size(html_content, context)
            
#         return chunks
    
#     def _extract_sections(self, soup: BeautifulSoup) -> List[str]:
#         """Extrai t√≠tulos de se√ß√µes do documento"""
#         headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
#         return [h.get_text().strip() for h in headers]
    
#     def _extract_context(self, soup: BeautifulSoup, text: str) -> Dict[str, Any]:
#         """Extrai contexto sem√¢ntico do documento usando IA"""
        
#         # Usa apenas os primeiros 2000 caracteres para an√°lise de contexto
#         sample_text = text[:2000]
        
#         prompt = f"""
#         Analise este trecho de documento acad√™mico e retorne um JSON com:
#         {{
#             "topics": ["t√≥pico1", "t√≥pico2", ...],
#             "style": "formal/informal/t√©cnico",
#             "level": "gradua√ß√£o/p√≥s-gradua√ß√£o/pesquisa"
#         }}
        
#         Texto: {sample_text}
#         """
        
#         try:
#             # Simula√ß√£o - em produ√ß√£o usar o agente Agno
#             return {
#                 "topics": ["tema principal", "metodologia"],
#                 "style": "formal",
#                 "level": "p√≥s-gradua√ß√£o"
#             }
#         except:
#             return {
#                 "topics": ["an√°lise acad√™mica"],
#                 "style": "formal", 
#                 "level": "gradua√ß√£o"
#             }
    
#     def _extract_title(self, soup: BeautifulSoup) -> str:
#         """Extrai t√≠tulo do documento"""
#         title_elem = soup.find('title') or soup.find('h1')
#         return title_elem.get_text().strip() if title_elem else "Documento Acad√™mico"
    
#     def _extract_abstract(self, soup: BeautifulSoup) -> str:
#         """Extrai resumo/abstract do documento"""
#         # Procura por elementos que possam ser o abstract
#         abstract_candidates = soup.find_all(['div', 'section', 'p'], 
#                                           class_=lambda x: x and ('abstract' in x.lower() or 'resumo' in x.lower()))
        
#         if abstract_candidates:
#             return abstract_candidates[0].get_text().strip()[:500]
        
#         # Fallback: primeiros par√°grafos
#         paragraphs = soup.find_all('p')[:3]
#         return ' '.join([p.get_text().strip() for p in paragraphs])[:500]
    
#     def _chunk_by_sections(self, soup: BeautifulSoup, sections: List, context: DocumentContext) -> List[DocumentChunk]:
#         """Divide documento por se√ß√µes l√≥gicas"""
#         chunks = []
        
#         for i, section in enumerate(sections):
#             # Encontra conte√∫do da se√ß√£o at√© a pr√≥xima
#             section_content = self._extract_section_content(soup, section, 
#                                                            sections[i+1] if i+1 < len(sections) else None)
            
#             if len(section_content) > self.CHUNK_SIZE:
#                 # Se se√ß√£o muito grande, divide em sub-chunks
#                 sub_chunks = self._split_large_section(section_content, section.get_text().strip())
#                 chunks.extend(sub_chunks)
#             else:
#                 chunk = DocumentChunk(
#                     id=f"section_{i}",
#                     content=section_content,
#                     position=i,
#                     section_title=section.get_text().strip(),
#                     context_summary=self._create_context_summary(context, section.get_text()),
#                     word_count=len(section_content.split())
#                 )
#                 chunks.append(chunk)
                
#         return chunks
    
#     def _chunk_by_size(self, html_content: str, context: DocumentContext) -> List[DocumentChunk]:
#         """Divide documento por tamanho com overlap inteligente"""
#         chunks = []
        
#         # Divide mantendo tags HTML √≠ntegras
#         soup = BeautifulSoup(html_content, 'html.parser')
#         paragraphs = soup.find_all(['p', 'div', 'section'])
        
#         current_chunk = ""
#         chunk_paras = []
        
#         for i, para in enumerate(paragraphs):
#             para_html = str(para)
            
#             if len(current_chunk + para_html) > self.CHUNK_SIZE and current_chunk:
#                 # Cria chunk atual
#                 chunk = DocumentChunk(
#                     id=f"chunk_{len(chunks)}",
#                     content=current_chunk,
#                     position=len(chunks),
#                     section_title=f"Se√ß√£o {len(chunks)+1}",
#                     context_summary=self._create_context_summary(context, current_chunk[:200]),
#                     word_count=len(current_chunk.split())
#                 )
#                 chunks.append(chunk)
                
#                 # Inicia novo chunk com overlap
#                 overlap_paras = chunk_paras[-2:] if len(chunk_paras) >= 2 else chunk_paras
#                 current_chunk = ''.join([str(p) for p in overlap_paras])
#                 chunk_paras = overlap_paras.copy()
            
#             current_chunk += para_html
#             chunk_paras.append(para)
        
#         # Adiciona √∫ltimo chunk
#         if current_chunk:
#             chunk = DocumentChunk(
#                 id=f"chunk_{len(chunks)}",
#                 content=current_chunk,
#                 position=len(chunks),
#                 section_title=f"Se√ß√£o {len(chunks)+1}",
#                 context_summary=self._create_context_summary(context, current_chunk[:200]),
#                 word_count=len(current_chunk.split())
#             )
#             chunks.append(chunk)
            
#         return chunks
    
#     def _extract_section_content(self, soup: BeautifulSoup, current_section, next_section) -> str:
#         """Extrai conte√∫do entre duas se√ß√µes"""
#         # Implementa√ß√£o simplificada - pega todos os elementos at√© a pr√≥xima se√ß√£o
#         content_elements = []
#         current = current_section.next_sibling
        
#         while current and current != next_section:
#             if hasattr(current, 'name') and current.name:
#                 content_elements.append(str(current))
#             current = current.next_sibling
            
#         return ''.join(content_elements)
    
#     def _split_large_section(self, content: str, section_title: str) -> List[DocumentChunk]:
#         """Divide se√ß√£o muito grande em sub-chunks"""
#         # Implementa√ß√£o similar ao _chunk_by_size
#         return []  # Simplificado para exemplo
    
#     def _create_context_summary(self, context: DocumentContext, sample_text: str) -> str:
#         """Cria resumo do contexto para o chunk"""
#         return f"Documento: {context.title}. T√≥picos: {', '.join(context.main_topics[:3])}. Estilo: {context.writing_style}."


# class AcademicDocumentService:
#     """Servi√ßo principal de revis√£o de documentos acad√™micos"""
    
#     def __init__(self, openai_api_key: str, postgres_config: Dict[str, Any]):
#         self.analyzer = DocumentAnalyzer()
        
#         # Base de conhecimento compartilhada
#         self.knowledge_base = PostgresKnowledgeBase(
#             host=postgres_config["host"],
#             port=postgres_config["port"],
#             user=postgres_config["user"],
#             password=postgres_config["password"],
#             database=postgres_config["database"]
#         )
        
#         # Agentes especializados
#         self._setup_agents(openai_api_key)
        
#     def _setup_agents(self, api_key: str):
#         """Configura os agentes especializados"""
        
#         # Agente para an√°lise de contexto global
#         self.context_agent = Agent(
#             name="DocumentContextAnalyzer",
#             model=OpenAIChat(id="gpt-4o-mini", api_key=api_key),
#             instructions="""
#             Voc√™ √© especialista em analisar contexto global de documentos acad√™micos.
#             Extraia informa√ß√µes estruturais, tem√°ticas e estil√≠sticas.
#             Retorne sempre JSON v√°lido com contexto resumido.
#             """,
#             knowledge=self.knowledge_base,
#             memory=True
#         )
        
#         # Agente revisor principal - focado no documento original
#         self.main_reviewer = Agent(
#             name="AcademicReviewer",
#             model=OpenAIChat(id="gpt-4o", api_key=api_key),
#             instructions=self._get_reviewer_instructions(),
#             knowledge=self.knowledge_base,
#             memory=True,
#             response_format="json"
#         )
        
#         # Agente para a√ß√µes globais (substitui√ß√µes em massa)
#         self.global_action_agent = Agent(
#             name="GlobalActionProcessor",
#             model=OpenAIChat(id="gpt-4o", api_key=api_key),
#             instructions="""
#             Voc√™ processa a√ß√µes globais em documentos acad√™micos (substitui√ß√µes, corre√ß√µes em massa).
#             Mant√©m formata√ß√£o HTML e estrutura do documento.
#             Retorna documento modificado completo.
#             """,
#             knowledge=self.knowledge_base,
#             memory=True
#         )
        
#         # Agente consolidador
#         self.consolidator_agent = Agent(
#             name="SuggestionConsolidator",
#             model=OpenAIChat(id="gpt-4o-mini", api_key=api_key),
#             instructions="""
#             Voc√™ consolida sugest√µes de m√∫ltiplos chunks em um resultado final coerente.
#             Remove duplicatas, resolve conflitos e organiza por prioridade.
#             Retorna JSON com formato especificado.
#             """,
#             knowledge=self.knowledge_base,
#             memory=True,
#             response_format="json"
#         )
    
#     def process_document(self, document_html: str, question: str) -> ProcessingResult:
#         """Processa documento acad√™mico com estrat√©gia otimizada"""
        
#         # 1. Analisa documento e determina estrat√©gia
#         strategy, context = self.analyzer.analyze_document(document_html)
        
#         # 2. Armazena contexto global na base de conhecimento
#         document_id = self._store_document_context(context, document_html)
        
#         # 3. Verifica se √© a√ß√£o global
#         is_global_action = self._is_global_action(question)
        
#         try:
#             if is_global_action:
#                 result = self._process_global_action(document_html, question, context)
#             elif strategy == ProcessingStrategy.SINGLE_PASS:
#                 result = self._process_single_pass(document_html, question, context)
#             else:
#                 result = self._process_chunked(document_html, question, context, strategy)
            
#             result.strategy_used = strategy
#             return result
            
#         finally:
#             # Limpa contexto temporal da sess√£o
#             self._cleanup_session_context(document_id)
    
#     def _is_global_action(self, question: str) -> bool:
#         """Detecta se a pergunta requer a√ß√£o global no documento"""
#         global_keywords = [
#             'todas as ocorr√™ncias', 'altere todas', 'substitua todos',
#             'mude todas', 'corrija todas', 'replace all', 'change all',
#             'documento inteiro', 'todo o documento', 'globalmente'
#         ]
        
#         question_lower = question.lower()
#         return any(keyword in question_lower for keyword in global_keywords)
    
#     def _process_global_action(self, document_html: str, question: str, context: DocumentContext) -> ProcessingResult:
#         """Processa a√ß√µes que afetam o documento inteiro"""
        
#         # Primeiro, identifica a a√ß√£o espec√≠fica
#         action_prompt = f"""
#         Contexto do documento: {context.title}
#         Pergunta do usu√°rio: {question}
        
#         Esta √© uma a√ß√£o global. Processe a seguinte a√ß√£o no documento completo:
        
#         Documento HTML:
#         {document_html}
        
#         Retorne JSON no formato:
#         {{
#             "suggestions": [{{
#                 "change": "documento HTML modificado completo",
#                 "explanation": "explica√ß√£o da altera√ß√£o global realizada"
#             }}],
#             "answer": "resposta explicando as altera√ß√µes feitas",
#             "modifiedDocument": "HTML com marca√ß√µes de altera√ß√£o"
#         }}
#         """
        
#         response = self.global_action_agent.run(action_prompt)
        
#         # Parse da resposta JSON
#         try:
#             result_data = json.loads(response.content if hasattr(response, 'content') else str(response))
            
#             suggestions = [
#                 Suggestion(
#                     change=s["change"],
#                     explanation=s["explanation"],
#                     global_action=True
#                 ) for s in result_data["suggestions"]
#             ]
            
#             return ProcessingResult(
#                 suggestions=suggestions,
#                 answer=result_data["answer"],
#                 modified_document=result_data["modifiedDocument"],
#                 strategy_used=ProcessingStrategy.SINGLE_PASS,
#                 cost_estimate=self._estimate_cost(len(document_html), "global_action")
#             )
            
#         except (json.JSONDecodeError, KeyError) as e:
#             # Fallback para formato texto
#             return ProcessingResult(
#                 suggestions=[Suggestion(
#                     change=document_html,
#                     explanation="Erro ao processar a√ß√£o global",
#                     global_action=True
#                 )],
#                 answer=f"Erro ao processar a√ß√£o global: {str(e)}",
#                 modified_document=document_html,
#                 strategy_used=ProcessingStrategy.SINGLE_PASS,
#                 cost_estimate=0.0
#             )
    
#     def _process_single_pass(self, document_html: str, question: str, context: DocumentContext) -> ProcessingResult:
#         """Processa documento pequeno em uma √∫nica passada"""
        
#         prompt = f"""
#         Voc√™ √© um especialista em revis√£o de documentos acad√™micos.
        
#         Contexto do documento:
#         - T√≠tulo: {context.title}
#         - N√≠vel acad√™mico: {context.academic_level}
#         - Estilo: {context.writing_style}
#         - Palavras: {context.total_words}
        
#         Documento HTML:
#         {document_html}
        
#         Pergunta: {question}
        
#         {self._get_output_format_instructions()}
#         """
        
#         response = self.main_reviewer.run(prompt)
        
#         return self._parse_agent_response(response, context, len(document_html))
    
#     def _process_chunked(self, document_html: str, question: str, context: DocumentContext, strategy: ProcessingStrategy) -> ProcessingResult:
#         """Processa documento grande usando chunks"""
        
#         # 1. Cria chunks
#         chunks = self.analyzer.create_chunks(document_html, context)
        
#         # 2. Processa cada chunk mantendo contexto
#         all_suggestions = []
#         chunk_results = []
        
#         for chunk in chunks:
#             chunk_result = self._process_single_chunk(chunk, question, context, chunks)
#             chunk_results.append(chunk_result)
#             all_suggestions.extend(chunk_result.suggestions)
        
#         # 3. Consolida resultados
#         consolidated_result = self._consolidate_chunk_results(
#             chunk_results, document_html, question, context
#         )
        
#         # 4. Reconstr√≥i documento com altera√ß√µes
#         modified_document = self._reconstruct_document_with_suggestions(
#             document_html, chunks, chunk_results
#         )
        
#         consolidated_result.modified_document = modified_document
#         consolidated_result.strategy_used = strategy
        
#         return consolidated_result
    
#     def _process_single_chunk(self, chunk: DocumentChunk, question: str, global_context: DocumentContext, all_chunks: List[DocumentChunk]) -> ProcessingResult:
#         """Processa um √∫nico chunk mantendo contexto global"""
        
#         # Contexto adicional dos chunks vizinhos
#         neighbor_context = self._get_neighbor_context(chunk, all_chunks)
        
#         prompt = f"""
#         Voc√™ est√° revisando parte de um documento acad√™mico. Mantenha o contexto global.
        
#         CONTEXTO GLOBAL:
#         - Documento: {global_context.title}
#         - N√≠vel: {global_context.academic_level}
#         - T√≥picos principais: {', '.join(global_context.main_topics)}
#         - Total de palavras: {global_context.total_words}
        
#         CONTEXTO LOCAL:
#         - Se√ß√£o: {chunk.section_title}
#         - Posi√ß√£o no documento: {chunk.position + 1} de {len(all_chunks)}
#         - Contexto vizinho: {neighbor_context}
        
#         CONTE√öDO ATUAL (Chunk {chunk.id}):
#         {chunk.content}
        
#         PERGUNTA: {question}
        
#         IMPORTANTE: Suas sugest√µes devem considerar o contexto global do documento.
#         N√£o fa√ßa altera√ß√µes que quebrem a coer√™ncia com outras se√ß√µes.
        
#         {self._get_output_format_instructions()}
#         """
        
#         response = self.main_reviewer.run(prompt)
#         result = self._parse_agent_response(response, global_context, len(chunk.content))
        
#         # Marca sugest√µes com ID do chunk
#         for suggestion in result.suggestions:
#             suggestion.chunk_id = chunk.id
            
#         return result
    
#     def _get_neighbor_context(self, current_chunk: DocumentChunk, all_chunks: List[DocumentChunk]) -> str:
#         """Obt√©m contexto dos chunks vizinhos"""
#         context_parts = []
        
#         # Chunk anterior
#         if current_chunk.position > 0:
#             prev_chunk = all_chunks[current_chunk.position - 1]
#             context_parts.append(f"Se√ß√£o anterior: {prev_chunk.section_title}")
        
#         # Chunk posterior
#         if current_chunk.position < len(all_chunks) - 1:
#             next_chunk = all_chunks[current_chunk.position + 1]
#             context_parts.append(f"Pr√≥xima se√ß√£o: {next_chunk.section_title}")
        
#         return " | ".join(context_parts) if context_parts else "Chunk isolado"
    
#     def _consolidate_chunk_results(self, chunk_results: List[ProcessingResult], original_html: str, question: str, context: DocumentContext) -> ProcessingResult:
#         """Consolida resultados de m√∫ltiplos chunks"""
        
#         # Combina todas as sugest√µes
#         all_suggestions = []
#         all_answers = []
        
#         for result in chunk_results:
#             all_suggestions.extend(result.suggestions)
#             all_answers.append(result.answer)
        
#         # Prompt para consolida√ß√£o
#         consolidation_prompt = f"""
#         Consolide as seguintes sugest√µes de revis√£o de um documento acad√™mico.
        
#         Documento: {context.title}
#         Pergunta original: {question}
        
#         Sugest√µes por chunk:
#         {json.dumps([{
#             'chunk_id': s.chunk_id,
#             'change': s.change[:200] + '...' if len(s.change) > 200 else s.change,
#             'explanation': s.explanation
#         } for s in all_suggestions], indent=2, ensure_ascii=False)}
        
#         Tarefas:
#         1. Remove sugest√µes duplicadas ou conflitantes
#         2. Prioriza sugest√µes mais impactantes
#         3. Garante coer√™ncia global
#         4. Cria resposta consolidada
        
#         {self._get_output_format_instructions()}
#         """
        
#         response = self.consolidator_agent.run(consolidation_prompt)
#         consolidated = self._parse_agent_response(response, context, len(original_html))
        
#         # Calcula custo total
#         total_cost = sum(result.cost_estimate for result in chunk_results)
#         consolidated.cost_estimate = total_cost
        
#         return consolidated
    
#     def _reconstruct_document_with_suggestions(self, original_html: str, chunks: List[DocumentChunk], chunk_results: List[ProcessingResult]) -> str:
#         """Reconstr√≥i documento aplicando sugest√µes dos chunks"""
        
#         # Cria mapa de sugest√µes por posi√ß√£o
#         suggestions_map = {}
#         suggestion_counter = 0
        
#         for i, result in enumerate(chunk_results):
#             chunk = chunks[i]
#             for suggestion in result.suggestions:
#                 if suggestion.change:
#                     suggestions_map[chunk.id] = {
#                         'original': chunk.content,
#                         'modified': suggestion.change,
#                         'index': suggestion_counter
#                     }
#                     suggestion_counter += 1
        
#         # Reconstr√≥i HTML marcando altera√ß√µes
#         modified_html = original_html
        
#         # Aplica marca√ß√µes de sugest√£o
#         for chunk_id, suggestion_data in suggestions_map.items():
#             original_content = suggestion_data['original']
#             modified_content = suggestion_data['modified']
#             index = suggestion_data['index']
            
#             # Encontra e substitui o conte√∫do original
#             if original_content in modified_html:
#                 marked_content = f'<suggestion data-idx="{index}">{original_content}</suggestion>'
#                 modified_html = modified_html.replace(original_content, marked_content, 1)
        
#         return modified_html
    
#     def _store_document_context(self, context: DocumentContext, document_html: str) -> str:
#         """Armazena contexto do documento na base de conhecimento"""
#         document_id = hashlib.md5(document_html.encode()).hexdigest()[:12]
        
#         context_data = {
#             "id": document_id,
#             "context": asdict(context),
#             "html_sample": document_html[:1000],  # Apenas amostra
#             "timestamp": "2024-01-01"  # Em produ√ß√£o usar datetime real
#         }
        
#         try:
#             self.knowledge_base.upsert([context_data])
#         except Exception as e:
#             print(f"Erro ao armazenar contexto: {e}")
        
#         return document_id
    
#     def _cleanup_session_context(self, document_id: str):
#         """Remove contexto temporal da sess√£o"""
#         try:
#             # Em produ√ß√£o, implementar limpeza da base de conhecimento
#             pass
#         except Exception as e:
#             print(f"Erro na limpeza: {e}")
    
#     def _parse_agent_response(self, response, context: DocumentContext, content_length: int) -> ProcessingResult:
#         """Converte resposta do agente para ProcessingResult"""
        
#         try:
#             # Tenta parsear como JSON
#             if hasattr(response, 'content'):
#                 response_text = response.content
#             else:
#                 response_text = str(response)
            
#             # Remove markdown se presente
#             if response_text.startswith('```json'):
#                 response_text = response_text.strip('```json').strip('```').strip()
            
#             data = json.loads(response_text)
            
#             suggestions = [
#                 Suggestion(
#                     change=s.get("change", ""),
#                     explanation=s.get("explanation", "")
#                 ) for s in data.get("suggestions", [])
#             ]
            
#             return ProcessingResult(
#                 suggestions=suggestions,
#                 answer=data.get("answer", "Processamento conclu√≠do"),
#                 modified_document=data.get("modifiedDocument", ""),
#                 strategy_used=ProcessingStrategy.SINGLE_PASS,
#                 cost_estimate=self._estimate_cost(content_length, "review")
#             )
            
#         except (json.JSONDecodeError, AttributeError) as e:
#             # Fallback para resposta em texto
#             return ProcessingResult(
#                 suggestions=[],
#                 answer=f"Erro ao processar resposta: {str(e)}",
#                 modified_document="",
#                 strategy_used=ProcessingStrategy.SINGLE_PASS,
#                 cost_estimate=0.0
#             )
    
#     def _estimate_cost(self, content_length: int, operation_type: str) -> float:
#         """Estima custo do processamento"""
        
#         # Estimativas baseadas em pre√ßos OpenAI (Janeiro 2024)
#         costs = {
#             "gpt-4o": {"input": 2.50, "output": 10.00},  # por 1M tokens
#             "gpt-4o-mini": {"input": 0.15, "output": 0.60}
#         }
        
#         # Estima tokens (aproximadamente 4 caracteres por token)
#         tokens = content_length / 4
        
#         if operation_type == "global_action":
#             # Usa GPT-4o para a√ß√µes globais
#             cost = (tokens * costs["gpt-4o"]["input"] + tokens * 0.5 * costs["gpt-4o"]["output"]) / 1_000_000
#         else:
#             # Usa mix de modelos
#             cost = (tokens * costs["gpt-4o-mini"]["input"] + tokens * 0.3 * costs["gpt-4o"]["output"]) / 1_000_000
        
#         return round(cost, 4)
    
#     def _get_reviewer_instructions(self) -> str:
#         """Retorna as instru√ß√µes do agente revisor (baseadas no prompt original)"""
#         return """
#         Voc√™ √© um **Especialista em Revis√£o de Documentos Acad√™micos e Cient√≠ficos** com expertise avan√ßada em:
#         - An√°lise cr√≠tica de conte√∫do acad√™mico (artigos, teses, monografias)
#         - Melhoria de clareza, legibilidade e estrutura textual
#         - Conformidade com normas acad√™micas e cient√≠ficas
#         - Sugest√µes de refer√™ncias e conte√∫do adicional
#         - Otimiza√ß√£o da organiza√ß√£o e fluxo de informa√ß√µes

#         CRIT√âRIOS DE AN√ÅLISE ACAD√äMICA:

#         Prioridades de Revis√£o:
#         1. **Clareza e Precis√£o**: Linguagem clara, terminologia adequada, aus√™ncia de ambiguidades
#         2. **Estrutura L√≥gica**: Organiza√ß√£o coerente, transi√ß√µes efetivas, hierarquia de ideias
#         3. **Rigor Acad√™mico**: Argumenta√ß√£o consistente, evid√™ncias adequadas, neutralidade cient√≠fica
#         4. **Normas e Conven√ß√µes**: Formata√ß√£o, cita√ß√µes, refer√™ncias, estrutura disciplinar
#         5. **Completude**: Lacunas de conte√∫do, necessidade de exemplos ou explica√ß√µes adicionais

#         Tipos de Sugest√µes Permitidas:
#         - **Melhoria de clareza**: Reformula√ß√£o de frases complexas ou amb√≠guas
#         - **Reorganiza√ß√£o estrutural**: Reordena√ß√£o de par√°grafos, se√ß√µes ou argumentos
#         - **Adi√ß√£o de conte√∫do**: Exemplos, explica√ß√µes, transi√ß√µes, refer√™ncias
#         - **Remo√ß√£o de redund√¢ncias**: Repeti√ß√µes, informa√ß√µes irrelevantes
#         - **Corre√ß√£o de fluxo**: Conex√µes l√≥gicas entre ideias
#         - **Adequa√ß√£o de registro**: Formalidade, tom acad√™mico apropriado

#         IMPORTANTE: Sempre considere o contexto global do documento ao fazer sugest√µes.
#         Mantenha coer√™ncia com o estilo e n√≠vel acad√™mico identificado.
#         """
    
#     def _get_output_format_instructions(self) -> str:
#         """Retorna instru√ß√µes de formato de sa√≠da JSON"""
#         return """
#         FORMATO DE SA√çDA OBRIGAT√ìRIO - Retorne EXCLUSIVAMENTE um JSON v√°lido:

#         {
#             "suggestions": [
#                 {
#                     "change": "HTML completo da altera√ß√£o sugerida",
#                     "explanation": "Justificativa t√©cnica e acad√™mica da sugest√£o"
#                 }
#             ],
#             "answer": "Resposta direta √† pergunta, incluindo explica√ß√µes das sugest√µes",
#             "modifiedDocument": "HTML modificado com tags <suggestion> ou null"
#         }

#         REGRAS PARA SUGEST√ïES:
#         - Cada sugest√£o deve abranger no m√°ximo elementos da raiz do HTML
#         - Mudan√ßas relacionadas = 1 sugest√£o
#         - Mudan√ßas independentes = sugest√µes separadas
#         - Preserve formata√ß√£o HTML original quando poss√≠vel
#         - Use tags <suggestion data-idx="i"> para marca√ß√µes
#         """


# # Exemplo de uso e teste da classe
# def create_service_example():
#     """Exemplo de como instanciar e usar o servi√ßo"""
    
#     # Configura√ß√£o
#     postgres_config = {
#         "host": "localhost",
#         "port": 5432,
#         "user": "agno_user",
#         "password": "password",
#         "database": "academic_docs"
#     }
    
#     openai_api_key = "sk-your-openai-key-here"
    
#     # Instancia o servi√ßo
#     service = AcademicDocumentService(openai_api_key, postgres_config)
    
#     return service


# def test_document_processing():
#     """Fun√ß√£o de teste com diferentes cen√°rios"""
    
#     service = create_service_example()
    
#     # Teste 1: Documento pequeno
#     small_document = """
#     <h1>Introdu√ß√£o ao Machine Learning</h1>
#     <p>Este trabalho analiza os principais algoritmos de aprendizado de m√°quina.</p>
#     <p>Os resultados s√£o importantes para a √°rea.</p>
#     """
    
#     result1 = service.process_document(
#         small_document, 
#         "Corrija erros ortogr√°ficos e melhore a conex√£o entre as ideias"
#     )
    
#     print("=== RESULTADO DOCUMENTO PEQUENO ===")
#     print(f"Estrat√©gia: {result1.strategy_used.value}")
#     print(f"Custo estimado: ${result1.cost_estimate}")
#     print(f"Sugest√µes: {len(result1.suggestions)}")
#     print(f"Resposta: {result1.answer[:200]}...")
#     print()
    
#     # Teste 2: A√ß√£o global
#     medium_document = """
#     <h1>An√°lise de Carros El√©tricos</h1>
#     <p>O carro el√©trico representa uma revolu√ß√£o no transporte.</p>
#     <h2>Tipos de Carro</h2>
#     <p>Existem diversos tipos de carro no mercado atual.</p>
#     <p>Cada carro possui caracter√≠sticas espec√≠ficas.</p>
#     """
    
#     result2 = service.process_document(
#         medium_document,
#         "Altere todas as ocorr√™ncias da palavra 'carro' por 've√≠culo'"
#     )
    
#     print("=== RESULTADO A√á√ÉO GLOBAL ===")
#     print(f"Estrat√©gia: {result2.strategy_used.value}")
#     print(f"Custo estimado: ${result2.cost_estimate}")
#     print(f"√â a√ß√£o global: {result2.suggestions[0].global_action if result2.suggestions else False}")
#     print(f"Resposta: {result2.answer[:200]}...")
#     print()
    
#     # Teste 3: Documento longo (simulado)
#     large_document = """
#     <h1>Tese de Doutorado: Intelig√™ncia Artificial na Medicina</h1>
#     <h2>Resumo</h2>
#     <p>Esta tese investiga aplica√ß√µes de IA na medicina moderna...</p>
#     """ + "<p>Par√°grafo de conte√∫do acad√™mico. " * 1000 + "</p>"
    
#     result3 = service.process_document(
#         large_document,
#         "Melhore a clareza e organiza√ß√£o do documento"
#     )
    
#     print("=== RESULTADO DOCUMENTO LONGO ===")
#     print(f"Estrat√©gia: {result3.strategy_used.value}")
#     print(f"Custo estimado: ${result3.cost_estimate}")
#     print(f"Sugest√µes: {len(result3.suggestions)}")
#     print(f"Documento modificado possui marca√ß√µes: {'<suggestion' in result3.modified_document}")
    
#     return [result1, result2, result3]


# # Classe utilit√°ria para integra√ß√£o com API REST
# class APIIntegrationHelper:
#     """Helper para integra√ß√£o com API REST FastAPI/Flask"""
    
#     def __init__(self, service: AcademicDocumentService):
#         self.service = service
    
#     def format_api_response(self, result: ProcessingResult) -> Dict[str, Any]:
#         """Converte ProcessingResult para formato da API original"""
        
#         return {
#             "suggestions": [
#                 {
#                     "change": suggestion.change,
#                     "explanation": suggestion.explanation
#                 } for suggestion in result.suggestions
#             ],
#             "answer": result.answer,
#             "modifiedDocument": result.modified_document,
#             "metadata": {
#                 "strategy_used": result.strategy_used.value,
#                 "cost_estimate": result.cost_estimate,
#                 "total_suggestions": len(result.suggestions)
#             }
#         }
    
#     def process_api_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
#         """Processa requisi√ß√£o da API no formato original"""
        
#         document_html = request_data.get("document", "")
#         question = request_data.get("question", "")
        
#         if not document_html or not question:
#             return {
#                 "error": "Documento e pergunta s√£o obrigat√≥rios",
#                 "suggestions": [],
#                 "answer": "",
#                 "modifiedDocument": None
#             }
        
#         try:
#             result = self.service.process_document(document_html, question)
#             return self.format_api_response(result)
            
#         except Exception as e:
#             return {
#                 "error": f"Erro no processamento: {str(e)}",
#                 "suggestions": [],
#                 "answer": f"Erro interno: {str(e)}",
#                 "modifiedDocument": None
#             }


# # Exemplo de uso com FastAPI (estrutura)
# """
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel

# app = FastAPI()

# class DocumentRequest(BaseModel):
#     document: str
#     question: str

# # Instancia o servi√ßo globalmente
# service = create_service_example()
# api_helper = APIIntegrationHelper(service)

# @app.post("/review-document")
# async def review_document(request: DocumentRequest):
#     try:
#         request_data = {
#             "document": request.document,
#             "question": request.question
#         }
        
#         result = api_helper.process_api_request(request_data)
        
#         if "error" in result:
#             raise HTTPException(status_code=400, detail=result["error"])
        
#         return result
        
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)
# """


# if __name__ == "__main__":
#     # Executa testes de exemplo
#     print("üöÄ Testando Servi√ßo de Revis√£o Acad√™mica com Agno")
#     print("=" * 60)
    
#     try:
#         results = test_document_processing()
#         print("\n‚úÖ Todos os testes executados com sucesso!")
#         print(f"üìä Resultados gerados: {len(results)}")
        
#     except Exception as e:
#         print(f"‚ùå Erro durante os testes: {e}")
#         print("üí° Certifique-se de que o Agno est√° instalado e configurado corretamente")